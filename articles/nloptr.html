<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Introduction to `nloptr`: an R interface to NLopt^[This package should be considered in beta and comments about any aspect of the package are welcome. This document is an R vignette prepared with the aid of `knitr` [@Xie:2014; @Xie:2015; @Xie:2016]. Financial support of the UK Economic and Social Research Council through a grant (RES-589-28-0001) to the ESRC Centre for Microdata Methods and Practice (CeMMAP) is gratefully acknowledged.] • nloptr</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Introduction to `nloptr`: an R interface to NLopt^[This package should be considered in beta and comments about any aspect of the package are welcome. This document is an R vignette prepared with the aid of `knitr` [@Xie:2014; @Xie:2015; @Xie:2016]. Financial support of the UK Economic and Social Research Council through a grant (RES-589-28-0001) to the ESRC Centre for Microdata Methods and Practice (CeMMAP) is gratefully acknowledged.]">
<meta property="og:description" content="nloptr">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">nloptr</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">2.0.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/nloptr.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/astamm/nloptr/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="nloptr_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Introduction to <code>nloptr</code>: an R interface to NLopt<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
</h1>
                        <h4 class="author">Jelmer Ypma</h4>
            
            <h4 class="date">2021-11-29</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/astamm/nloptr/blob/master/vignettes/nloptr.Rmd"><code>vignettes/nloptr.Rmd</code></a></small>
      <div class="hidden name"><code>nloptr.Rmd</code></div>

    </div>

    
    
<p>This document describes how to use <code>nloptr</code>, which is an R interface to NLopt. NLopt is a free/open-source library for nonlinear optimization started by Steven G. Johnson, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms. The NLopt library is available under the GNU Lesser General Public License (LGPL), and the copyrights are owned by a variety of authors.</p>
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>NLopt addresses general nonlinear optimization problems of the form: <span class="math display">\[
\begin{aligned}
&amp;\min_{x \in R^n} f(x) \\
s.t.&amp; g(x) \leq 0 \\
&amp; h(x) = 0 \\
&amp; x_L \leq x \leq x_U
\end{aligned}
\]</span> where <span class="math inline">\(f(\cdot)\)</span> is the objective function and <span class="math inline">\(x\)</span> represents the <span class="math inline">\(n\)</span> optimization parameters. This problem may optionally be subject to the bound constraints (also called box constraints), <span class="math inline">\(x_L\)</span> and <span class="math inline">\(x_U\)</span>. For partially or totally unconstrained problems the bounds can take values <span class="math inline">\(-\infty\)</span> or <span class="math inline">\(\infty\)</span>. One may also optionally have <span class="math inline">\(m\)</span> nonlinear inequality constraints (sometimes called a nonlinear programming problem), which can be specified in <span class="math inline">\(g(\cdot)\)</span>, and equality constraints that can be specified in <span class="math inline">\(h(\cdot)\)</span>. Note that not all of the algorithms in NLopt can handle constraints.</p>
<p>This vignette describes how to formulate minimization problems to be solved with the R interface to NLopt. If you want to use the C interface directly or are interested in the Matlab interface, there are other sources of documentation avialable. Some of the information here has been taken from the <a href="%5Ctexttt%7Bhttp://ab-initio.mit.edu/nlopt">NLopt website</a>, where more details are available. All credit for implementing the C code for the different algorithms available in NLopt should go to the respective authors. See the <a href="http://ab-initio.mit.edu/wiki/index.php/Citing_NLopt">website</a> for information on how to cite NLopt and the algorithms you use.</p>
</div>
<div id="installation" class="section level1">
<h1 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h1>
<p>This package is on CRAN and can be installed from within R using</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"nloptr"</span><span class="op">)</span></code></pre></div>
<p>You should now be able to load the R interface to NLopt and read the help.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://astamm.github.io/nloptr/index.html">'nloptr'</a></span><span class="op">)</span>
<span class="op">?</span><span class="va">nloptr</span></code></pre></div>
<p>The most recent experimental version of <code>nloptr</code> can be installed from R-Forge using</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"nloptr"</span>,repos<span class="op">=</span><span class="st">"http://R-Forge.R-project.org"</span><span class="op">)</span></code></pre></div>
<p>or from source using</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"nloptr"</span>,type<span class="op">=</span><span class="st">"source"</span>,repos<span class="op">=</span><span class="st">"http://R-Forge.R-project.org"</span><span class="op">)</span></code></pre></div>
</div>
<div id="minimizing-the-rosenbrock-banana-function" class="section level1">
<h1 class="hasAnchor">
<a href="#minimizing-the-rosenbrock-banana-function" class="anchor"></a>Minimizing the Rosenbrock Banana function</h1>
<p>As a first example we will solve an unconstrained minimization problem. The function we look at is the Rosenbrock Banana function <span class="math display">\[
f( x ) = 100 \left( x_2 - x_1^2 \right)^2 + \left(1 - x_1 \right)^2,
\]</span> which is also used as an example in the documentation for the standard R optimizer <code>optim</code>. The gradient of the objective function is given by <span class="math display">\[
\nabla f( x ) = 
\left( \begin{array}[1]{c}
-400 \cdot x_1 \cdot (x_2 - x_1^2) - 2 \cdot (1 - x_1) \\
 200 \cdot (x_2 - x_1^2)
\end{array} \right).
\]</span> Not all of the algorithms in NLopt need gradients to be supplied by the user. We will show examples with and without supplying the gradient. After loading the library</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://astamm.github.io/nloptr/index.html">nloptr</a></span><span class="op">)</span></code></pre></div>
<p>we start by specifying the objective function and its gradient</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Rosenbrock Banana function</span>
<span class="va">eval_f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span>   
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span> <span class="fl">100</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">)</span>
<span class="op">}</span>

<span class="co">## Gradient of Rosenbrock Banana function</span>
<span class="va">eval_grad_f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span> 
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="op">-</span><span class="fl">400</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,
                <span class="fl">200</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">)</span> <span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>We define initial values</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># initial values</span>
<span class="va">x0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="op">-</span><span class="fl">1.2</span>, <span class="fl">1</span> <span class="op">)</span></code></pre></div>
<p>and then minimize the function using the <code>nloptr</code> command. This command runs some checks on the supplied inputs and returns an object with the exit code of the solver, the optimal value of the objective function and the solution. Before we can minimize the function we need to specify which algorithm we want to use</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">opts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"algorithm"</span><span class="op">=</span><span class="st">"NLOPT_LD_LBFGS"</span>,
             <span class="st">"xtol_rel"</span><span class="op">=</span><span class="fl">1.0e-8</span><span class="op">)</span></code></pre></div>
<p>Here we use the L-BFGS algorithm <span class="citation">(Nocedal 1980; Liu and Nocedal 1989)</span>. The characters <code>LD</code> in the algorithm show that this algorithm looks for local minima (<code>L</code>) using a derivative-based (<code>D</code>) algorithm. Other algorithms look for global (<code>G</code>) minima, or they don’t need derivatives (<code>N</code>). We also specified the termination criterium in terms of the relative x-tolerance. Other termination criteria are available (see Appendix <code>\ref{sec:descoptions}</code> for a full list of options). We then solve the minimization problem using</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># solve Rosenbrock Banana function</span>
<span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nloptr.html">nloptr</a></span><span class="op">(</span> x0<span class="op">=</span><span class="va">x0</span>, 
               eval_f<span class="op">=</span><span class="va">eval_f</span>, 
               eval_grad_f<span class="op">=</span><span class="va">eval_grad_f</span>,
               opts<span class="op">=</span><span class="va">opts</span><span class="op">)</span></code></pre></div>
<p>We can see the results by printing the resulting object.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span> <span class="va">res</span> <span class="op">)</span></code></pre></div>
<pre><code>## 
## Call:
## 
## nloptr(x0 = x0, eval_f = eval_f, eval_grad_f = eval_grad_f, opts = opts)
## 
## 
## 
## Minimization using NLopt version 2.7.0 
## 
## NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success 
## return value. )
## 
## Number of Iterations....: 56 
## Termination conditions:  xtol_rel: 1e-08 
## Number of inequality constraints:  0 
## Number of equality constraints:    0 
## Optimal value of objective function:  7.35727226897802e-23 
## Optimal value of controls: 1 1</code></pre>
<p>Sometimes the objective function and its gradient contain common terms. To economize on calculations, we can return the objective and its gradient in a list. For the Rosenbrock Banana function we have for instance</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Rosenbrock Banana function and gradient in one function</span>
<span class="va">eval_f_list</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">common_term</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span> <span class="st">"objective"</span> <span class="op">=</span> <span class="fl">100</span> <span class="op">*</span> <span class="va">common_term</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>,
                  <span class="st">"gradient"</span>  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="op">-</span><span class="fl">400</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="va">common_term</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,
                                    <span class="fl">200</span> <span class="op">*</span> <span class="va">common_term</span><span class="op">)</span> <span class="op">)</span> <span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>which we minimize using</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nloptr.html">nloptr</a></span><span class="op">(</span> x0<span class="op">=</span><span class="va">x0</span>, 
               eval_f<span class="op">=</span><span class="va">eval_f_list</span>,
               opts<span class="op">=</span><span class="va">opts</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span> <span class="va">res</span> <span class="op">)</span></code></pre></div>
<pre><code>## 
## Call:
## nloptr(x0 = x0, eval_f = eval_f_list, opts = opts)
## 
## 
## Minimization using NLopt version 2.7.0 
## 
## NLopt solver status: 1 ( NLOPT_SUCCESS: Generic success 
## return value. )
## 
## Number of Iterations....: 56 
## Termination conditions:  xtol_rel: 1e-08 
## Number of inequality constraints:  0 
## Number of equality constraints:    0 
## Optimal value of objective function:  7.35727226897802e-23 
## Optimal value of controls: 1 1</code></pre>
<p>This gives the same results as before.</p>
</div>
<div id="minimization-with-inequality-constraints" class="section level1">
<h1 class="hasAnchor">
<a href="#minimization-with-inequality-constraints" class="anchor"></a>Minimization with inequality constraints</h1>
<p>This section shows how to minimize a function subject to inequality constraints. This example is the same as the one used in the tutorial on the NLopt website. The problem we want to solve is <span class="math display">\[
\begin{aligned}
&amp;\min_{x \in R^n} \sqrt{x_2} \\
s.t.&amp; x_2 \geq 0 \\
&amp; x_2 \geq ( a_1 x_1 + b_1 )^3 \\
&amp; x_2 \geq ( a_2 x_1 + b_2 )^3, 
\end{aligned}
\]</span> where <span class="math inline">\(a_1 = 2\)</span>, <span class="math inline">\(b_1 = 0\)</span>, <span class="math inline">\(a_2 = -1\)</span>, and <span class="math inline">\(b_2 = 1\)</span>. In order to solve this problem, we first have to re-formulate the constraints to be of the form <span class="math inline">\(g(x) \leq 0\)</span>. Note that the first constraint is a bound on <span class="math inline">\(x_2\)</span>, which we will add later. The other two constraints can be re-written as <span class="math display">\[
\begin{aligned}
( a_1 x_1 + b_1 )^3 - x_2 &amp;\leq 0 \\
( a_2 x_1 + b_2 )^3 - x_2 &amp;\leq 0
\end{aligned}
\]</span></p>
<p>First we define R functions to calculate the objective function and its gradient</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># objective function</span>
<span class="va">eval_f0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span> <span class="va">x</span>, <span class="va">a</span>, <span class="va">b</span> <span class="op">)</span><span class="op">{</span> 
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">)</span>
<span class="op">}</span>

<span class="co"># gradient of objective function</span>
<span class="va">eval_grad_f0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span> <span class="va">x</span>, <span class="va">a</span>, <span class="va">b</span> <span class="op">)</span><span class="op">{</span> 
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">.5</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">)</span> <span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>If needed, these can of course be calculated in the same function as before. Then we define the two constraints and the jacobian of the constraints</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># constraint function</span>
<span class="va">eval_g0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span> <span class="va">x</span>, <span class="va">a</span>, <span class="va">b</span> <span class="op">)</span> <span class="op">{</span>
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span> <span class="op">(</span><span class="va">a</span><span class="op">*</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">b</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">)</span>
<span class="op">}</span>

<span class="co"># jacobian of constraint</span>
<span class="va">eval_jac_g0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span> <span class="va">x</span>, <span class="va">a</span>, <span class="va">b</span> <span class="op">)</span> <span class="op">{</span>
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">3</span><span class="op">*</span><span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">*</span><span class="op">(</span><span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">*</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">b</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>, <span class="op">-</span><span class="fl">1.0</span> <span class="op">)</span>, 
                   <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">3</span><span class="op">*</span><span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">*</span><span class="op">(</span><span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">*</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">b</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>, <span class="op">-</span><span class="fl">1.0</span> <span class="op">)</span> <span class="op">)</span> <span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>Note that all of the functions above depend on additional parameters, <code>a</code> and <code>b</code>. We have to supply specific values for these when we invoke the optimization command. The constraint function <code>eval_g0</code> returns a vector with in this case the same length as the vectors <code>a</code> and <code>b</code>. The function calculating the jacobian of the constraint should return a matrix where the number of rows equal the number of constraints (in this case two). The number of columns should equal the number of control variables (two in this case as well).</p>
<p>After defining values for the parameters</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># define parameters</span>
<span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="op">-</span><span class="fl">1</span><span class="op">)</span>
<span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></code></pre></div>
<p>we can minimize the function subject to the constraints with the following command</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Solve using NLOPT_LD_MMA with gradient information supplied in separate function</span>
<span class="va">res0</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nloptr.html">nloptr</a></span><span class="op">(</span> x0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.234</span>,<span class="fl">5.678</span><span class="op">)</span>, 
                eval_f<span class="op">=</span><span class="va">eval_f0</span>, 
                eval_grad_f<span class="op">=</span><span class="va">eval_grad_f0</span>,
                lb <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="cn">Inf</span>,<span class="fl">0</span><span class="op">)</span>, 
                ub <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">Inf</span>,<span class="cn">Inf</span><span class="op">)</span>, 
                eval_g_ineq <span class="op">=</span> <span class="va">eval_g0</span>,
                eval_jac_g_ineq <span class="op">=</span> <span class="va">eval_jac_g0</span>,                
                opts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"algorithm"</span> <span class="op">=</span> <span class="st">"NLOPT_LD_MMA"</span>,
                            <span class="st">"xtol_rel"</span><span class="op">=</span><span class="fl">1.0e-8</span>,
                            <span class="st">"print_level"</span> <span class="op">=</span> <span class="fl">2</span>,
                            <span class="st">"check_derivatives"</span> <span class="op">=</span> <span class="cn">TRUE</span>,
                            <span class="st">"check_derivatives_print"</span> <span class="op">=</span> <span class="st">"all"</span><span class="op">)</span>,
                a <span class="op">=</span> <span class="va">a</span>, 
                b <span class="op">=</span> <span class="va">b</span> <span class="op">)</span></code></pre></div>
<pre><code>## Checking gradients of objective function.</code></pre>
<pre><code>## Derivative checker results: 0 error(s) detected.</code></pre>
<pre><code>## 
##   eval_grad_f[ 1 ] = 0.000000e+00 ~ 0.000000e+00   [0.000000e+00]
##   eval_grad_f[ 2 ] = 2.098323e-01 ~ 2.098323e-01   [1.422937e-09]</code></pre>
<pre><code>## Checking gradients of inequality constraints.</code></pre>
<pre><code>## Derivative checker results: 0 error(s) detected.</code></pre>
<pre><code>## 
##   eval_jac_g_ineq[ 1, 1 ] =  3.654614e+01 ~  3.654614e+01   [1.667794e-08]
##   eval_jac_g_ineq[ 2, 1 ] = -1.642680e-01 ~ -1.642680e-01   [2.103453e-07]
##   eval_jac_g_ineq[ 1, 2 ] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00]
##   eval_jac_g_ineq[ 2, 2 ] = -1.000000e+00 ~ -1.000000e+00   [0.000000e+00]</code></pre>
<pre><code>## iteration: 1
##  f(x) = 2.382855
##  g(x) = ( 9.354647, -5.690813 )
## iteration: 2
##  f(x) = 2.356135
##  g(x) = ( -0.122988, -5.549587 )
## iteration: 3
##  f(x) = 2.245864
##  g(x) = ( -0.531886, -5.038655 )
## iteration: 4
##  f(x) = 2.019102
##  g(x) = ( -3.225103, -3.931195 )
## iteration: 5
##  f(x) = 1.740934
##  g(x) = ( -2.676263, -2.761136 )
## iteration: 6
##  f(x) = 1.404206
##  g(x) = ( -1.674055, -1.676216 )
## iteration: 7
##  f(x) = 1.022295
##  g(x) = ( -0.748790, -0.748792 )
## iteration: 8
##  f(x) = 0.685203
##  g(x) = ( -0.173206, -0.173207 )
## iteration: 9
##  f(x) = 0.552985
##  g(x) = ( -0.009496, -0.009496 )
## iteration: 10
##  f(x) = 0.544354
##  g(x) = ( -0.000025, -0.000025 )
## iteration: 11
##  f(x) = 0.544331
##  g(x) = ( 0.000000, 0.000000 )
## iteration: 12
##  f(x) = 0.544331
##  g(x) = ( -0.000000, 0.000000 )
## iteration: 13
##  f(x) = 0.544331
##  g(x) = ( -0.000000, 0.000000 )
## iteration: 14
##  f(x) = 0.544331
##  g(x) = ( -0.000000, 0.000000 )
## iteration: 15
##  f(x) = 0.544331
##  g(x) = ( -0.000000, 0.000000 )
## iteration: 16
##  f(x) = 0.544331
##  g(x) = ( -0.000000, 0.000000 )
## iteration: 17
##  f(x) = 0.544331
##  g(x) = ( -0.000000, 0.000000 )
## iteration: 18
##  f(x) = 0.544331
##  g(x) = ( -0.000000, 0.000000 )
## iteration: 19
##  f(x) = 0.544331
##  g(x) = ( 0.000000, 0.000000 )
## iteration: 20
##  f(x) = 0.544331
##  g(x) = ( -0.000000, -0.000000 )
## iteration: 21
##  f(x) = 0.544331
##  g(x) = ( 0.000000, 0.000000 )</code></pre>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span> <span class="va">res0</span> <span class="op">)</span></code></pre></div>
<pre><code>## 
## Call:
## 
## nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, eval_grad_f = eval_grad_f0, 
##     lb = c(-Inf, 0), ub = c(Inf, Inf), eval_g_ineq = eval_g0, 
##     eval_jac_g_ineq = eval_jac_g0, opts = list(algorithm = "NLOPT_LD_MMA", 
##         xtol_rel = 1e-08, print_level = 2, check_derivatives = TRUE, 
##         check_derivatives_print = "all"), a = a, b = b)
## 
## 
## Minimization using NLopt version 2.7.0 
## 
## NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization 
## stopped because xtol_rel or xtol_abs (above) was reached. 
## )
## 
## Number of Iterations....: 21 
## Termination conditions:  xtol_rel: 1e-08 
## Number of inequality constraints:  2 
## Number of equality constraints:    0 
## Optimal value of objective function:  0.54433104762009 
## Optimal value of controls: 0.3333333 0.2962963</code></pre>
<p>Here we supplied lower bounds for <span class="math inline">\(x_2\)</span> in <code>lb</code>. There are no upper bounds for both control variables, so we supply <code>Inf</code> values. If we don’t supply lower or upper bounds, plus or minus infinity is chosen by default. The inequality constraints and its jacobian are defined using <code>eval_g_ineq</code> and <code>eval_jac_g_ineq</code>. Not all algorithms can handle inequality constraints, so we have to specifiy one that does, <code>NLOPT_LD_MMA</code> <span class="citation">(Svanberg 2002)</span>.</p>
<p>We also specify the option <code>print_level</code> to obtain output during the optimization process. For the available <code>print_level</code> values, see <code><a href="../reference/nloptr.html">?nloptr</a></code>. Setting the <code>check_derivatives</code> option to <code>TRUE</code>, compares the gradients supplied by the user with a finite difference approximation in the initial point (<code>x0</code>). When this check is run, the option <code>check_derivatives_print</code> can be used to print all values of the derivative checker (<code>all</code> (default)), only those values that result in an error (<code>errors</code>) or no output (<code>none</code>), in which case only the number of errors is shown. The tolerance that determines if a difference between the analytic gradient and the finite difference approximation results in an error can be set using the option <code>check_derivatives_tol</code> (default = 1e-04). The first column shows the value of the analytic gradient, the second column shows the value of the finite difference approximation, and the third column shows the relative error. Stars are added at the front of a line if the relative error is larger than the specified tolerance.</p>
<p>Finally, we add all the parameters that have to be passed on to the objective and constraint functions, <code>a</code> and <code>b</code>.</p>
<p>We can also use a different algorithm to solve the same minimization problem. The only thing we have to change is the algorithm that we want to use, in this case <code>NLOPT_LN_COBYLA</code>, which is an algorithm that doesn’t need gradient information <span class="citation">(Powell 1994, 1998)</span>.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Solve using NLOPT_LN_COBYLA without gradient information</span>
<span class="va">res1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/nloptr.html">nloptr</a></span><span class="op">(</span> x0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.234</span>,<span class="fl">5.678</span><span class="op">)</span>, 
                eval_f<span class="op">=</span><span class="va">eval_f0</span>, 
                lb <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="cn">Inf</span>,<span class="fl">0</span><span class="op">)</span>, 
                ub <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">Inf</span>,<span class="cn">Inf</span><span class="op">)</span>, 
                eval_g_ineq <span class="op">=</span> <span class="va">eval_g0</span>, 
                opts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"algorithm"</span><span class="op">=</span><span class="st">"NLOPT_LN_COBYLA"</span>, 
                            <span class="st">"xtol_rel"</span><span class="op">=</span><span class="fl">1.0e-8</span><span class="op">)</span>,
                a <span class="op">=</span> <span class="va">a</span>, 
                b <span class="op">=</span> <span class="va">b</span> <span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span> <span class="va">res1</span> <span class="op">)</span></code></pre></div>
<pre><code>## 
## Call:
## 
## nloptr(x0 = c(1.234, 5.678), eval_f = eval_f0, lb = c(-Inf, 0), 
##     ub = c(Inf, Inf), eval_g_ineq = eval_g0, opts = list(algorithm = "NLOPT_LN_COBYLA", 
##         xtol_rel = 1e-08), a = a, b = b)
## 
## 
## Minimization using NLopt version 2.7.0 
## 
## NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization 
## stopped because xtol_rel or xtol_abs (above) was reached. 
## )
## 
## Number of Iterations....: 50 
## Termination conditions:  xtol_rel: 1e-08 
## Number of inequality constraints:  2 
## Number of equality constraints:    0 
## Optimal value of objective function:  0.544331053951819 
## Optimal value of controls: 0.3333333 0.2962963</code></pre>
</div>
<div id="derivative-checker" class="section level1">
<h1 class="hasAnchor">
<a href="#derivative-checker" class="anchor"></a>Derivative checker</h1>
<p>The derivative checker can be called when supplying a minimization problem to <code>nloptr</code>, using the options <code>check_derivatives</code>, <code>check_derivatives_tol</code> and <code>check_derivatives_print</code>, but it can also be used separately. For example, define the function <code>g</code>, with vector outcome, and its gradient <code>g_grad</code></p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">g</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span> <span class="va">x</span>, <span class="va">a</span> <span class="op">)</span> <span class="op">{</span>
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span> 
        <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, 
           <span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,
          <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>, 
          <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>, 
          <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span>,
          <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span> 
        <span class="op">)</span> 
    <span class="op">)</span>
<span class="op">}</span>

<span class="va">g_grad</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span> <span class="va">x</span>, <span class="va">a</span> <span class="op">)</span> <span class="op">{</span>
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span> 
        <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span> 
            <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">1</span>, <span class="fl">0</span> <span class="op">)</span>,
            <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">1</span> <span class="op">)</span>,
            <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>, <span class="fl">0</span> <span class="op">)</span>,
            <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>, <span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">)</span>,
            <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">3</span><span class="op">*</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>, <span class="fl">0</span> <span class="op">)</span>,
            <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fl">0</span>, <span class="fl">3</span><span class="op">*</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">-</span> <span class="va">a</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">)</span>
        <span class="op">)</span>
    <span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p><code>a</code> is some vector containing data. The gradient contains some errors in this case. By calling the function <code>check.derivatives</code> we can check the user-supplied analytic gradients with a finite difference approximation at a point <code>.x</code>.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/check.derivatives.html">check.derivatives</a></span><span class="op">(</span> 
            .x<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span>, 
            func<span class="op">=</span><span class="va">g</span>, 
            func_grad<span class="op">=</span><span class="va">g_grad</span>, 
            check_derivatives_print<span class="op">=</span><span class="st">'all'</span>, 
            a<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.3</span>, <span class="fl">.8</span><span class="op">)</span> <span class="op">)</span></code></pre></div>
<pre><code>## Derivative checker results: 2 error(s) detected.</code></pre>
<pre><code>## 
##   grad_f[ 1, 1 ] = 1.00e+00 ~ 1.00e+00   [0.000000e+00]
##   grad_f[ 2, 1 ] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]
##   grad_f[ 3, 1 ] = 1.40e+00 ~ 1.40e+00   [9.579318e-09]
## * grad_f[ 4, 1 ] = 1.40e+00 ~ 0.00e+00   [1.400000e+00]
## * grad_f[ 5, 1 ] = 1.20e-01 ~ 1.47e+00   [9.183673e-01]
##   grad_f[ 6, 1 ] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]
##   grad_f[ 1, 2 ] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]
##   grad_f[ 2, 2 ] = 1.00e+00 ~ 1.00e+00   [0.000000e+00]
##   grad_f[ 3, 2 ] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]
##   grad_f[ 4, 2 ] = 2.40e+00 ~ 2.40e+00   [1.179675e-08]
##   grad_f[ 5, 2 ] = 0.00e+00 ~ 0.00e+00   [0.000000e+00]
##   grad_f[ 6, 2 ] = 4.32e+00 ~ 4.32e+00   [2.593906e-08]</code></pre>
<p>The errors are shown on screen, where the option <code>check_derivatives_print</code> determines the amount of output you see. The value of the analytic gradient and the value of the finite difference approximation at the supplied point is returned in a list.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res</span></code></pre></div>
<pre><code>## $analytic
##      [,1] [,2]
## [1,] 1.00 0.00
## [2,] 0.00 1.00
## [3,] 1.40 0.00
## [4,] 1.40 2.40
## [5,] 0.12 0.00
## [6,] 0.00 4.32
## 
## $finite_difference
##      [,1] [,2]
## [1,] 1.00 0.00
## [2,] 0.00 1.00
## [3,] 1.40 0.00
## [4,] 0.00 2.40
## [5,] 1.47 0.00
## [6,] 0.00 4.32
## 
## $relative_error
##              [,1]         [,2]
## [1,] 0.000000e+00 0.000000e+00
## [2,] 0.000000e+00 0.000000e+00
## [3,] 9.579318e-09 0.000000e+00
## [4,] 1.400000e+00 1.179675e-08
## [5,] 9.183673e-01 0.000000e+00
## [6,] 0.000000e+00 2.593906e-08
## 
## $flag_derivative_warning
##       [,1]  [,2]
## [1,] FALSE FALSE
## [2,] FALSE FALSE
## [3,] FALSE FALSE
## [4,]  TRUE FALSE
## [5,]  TRUE FALSE
## [6,] FALSE FALSE</code></pre>
<p>Note that not all errors will be picked up by the derivative checker. For instance, if we run the check with <code>a = c(.5, .5)</code>, one of the errors is not flagged as an error.</p>
</div>
<div id="notes" class="section level1">
<h1 class="hasAnchor">
<a href="#notes" class="anchor"></a>Notes</h1>
<p>The <code>.R</code> scripts in the <code>tests</code> directory contain more examples. For instance, <code>hs071.R</code> and <code>systemofeq.R</code> show how to solve problems with equality constraints. See the <a href="http://ab-initio.mit.edu/wiki/index.php/NLopt_Algorithms#Augmented_Lagrangian_algorithm">NLopt website</a> for more details. Please let me know if you’re missing any of the features that are implemented in NLopt.</p>
<p>Sometimes the optimization procedure terminates with a message <code>maxtime was reached</code> without evaluating the objective function. Submitting the same problem again usually solves this problem.</p>
</div>
<div id="description-of-options" class="section level1">
<h1 class="hasAnchor">
<a href="#description-of-options" class="anchor"></a>Description of options</h1>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">nloptr</span><span class="fu">::</span><span class="fu"><a href="../reference/nloptr.print.options.html">nloptr.print.options</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## algorithm
##  possible values: NLOPT_GN_DIRECT, NLOPT_GN_DIRECT_L,
##                   NLOPT_GN_DIRECT_L_RAND, NLOPT_GN_DIRECT_NOSCAL,
##                   NLOPT_GN_DIRECT_L_NOSCAL,
##                   NLOPT_GN_DIRECT_L_RAND_NOSCAL,
##                   NLOPT_GN_ORIG_DIRECT, NLOPT_GN_ORIG_DIRECT_L,
##                   NLOPT_GD_STOGO, NLOPT_GD_STOGO_RAND,
##                   NLOPT_LD_SLSQP, NLOPT_LD_LBFGS_NOCEDAL,
##                   NLOPT_LD_LBFGS, NLOPT_LN_PRAXIS, NLOPT_LD_VAR1,
##                   NLOPT_LD_VAR2, NLOPT_LD_TNEWTON,
##                   NLOPT_LD_TNEWTON_RESTART,
##                   NLOPT_LD_TNEWTON_PRECOND,
##                   NLOPT_LD_TNEWTON_PRECOND_RESTART,
##                   NLOPT_GN_CRS2_LM, NLOPT_GN_MLSL, NLOPT_GD_MLSL,
##                   NLOPT_GN_MLSL_LDS, NLOPT_GD_MLSL_LDS,
##                   NLOPT_LD_MMA, NLOPT_LD_CCSAQ, NLOPT_LN_COBYLA,
##                   NLOPT_LN_NEWUOA, NLOPT_LN_NEWUOA_BOUND,
##                   NLOPT_LN_NELDERMEAD, NLOPT_LN_SBPLX,
##                   NLOPT_LN_AUGLAG, NLOPT_LD_AUGLAG,
##                   NLOPT_LN_AUGLAG_EQ, NLOPT_LD_AUGLAG_EQ,
##                   NLOPT_LN_BOBYQA, NLOPT_GN_ISRES
##  default value:   none
## 
##  This option is required. Check the NLopt website for a description of
##  the algorithms.
## 
## stopval
##  possible values: -Inf &lt;= stopval &lt;= Inf
##  default value:   -Inf
## 
##  Stop minimization when an objective value &lt;= stopval is found.
##  Setting stopval to -Inf disables this stopping criterion (default).
## 
## ftol_rel
##  possible values: ftol_rel &gt; 0
##  default value:   0.0
## 
##  Stop when an optimization step (or an estimate of the optimum)
##  changes the objective function value by less than ftol_rel multiplied
##  by the absolute value of the function value. If there is any chance
##  that your optimum function value is close to zero, you might want to
##  set an absolute tolerance with ftol_abs as well. Criterion is
##  disabled if ftol_rel is non-positive (default).
## 
## ftol_abs
##  possible values: ftol_abs &gt; 0
##  default value:   0.0
## 
##  Stop when an optimization step (or an estimate of the optimum)
##  changes the function value by less than ftol_abs. Criterion is
##  disabled if ftol_abs is non-positive (default).
## 
## xtol_rel
##  possible values: xtol_rel &gt; 0
##  default value:   1.0e-04
## 
##  Stop when an optimization step (or an estimate of the optimum)
##  changes every parameter by less than xtol_rel multiplied by the
##  absolute value of the parameter. If there is any chance that an
##  optimal parameter is close to zero, you might want to set an absolute
##  tolerance with xtol_abs as well. Criterion is disabled if xtol_rel is
##  non-positive.
## 
## xtol_abs
##  possible values: xtol_abs &gt; 0
##  default value:   rep( 0.0, length(x0) )
## 
##  xtol_abs is a vector of length n (the number of elements in x) giving
##  the tolerances: stop when an optimization step (or an estimate of the
##  optimum) changes every parameter x[i] by less than xtol_abs[i].
##  Criterion is disabled if all elements of xtol_abs are non-positive
##  (default).
## 
## maxeval
##  possible values: maxeval is a positive integer
##  default value:   100
## 
##  Stop when the number of function evaluations exceeds maxeval. This is
##  not a strict maximum: the number of function evaluations may exceed
##  maxeval slightly, depending upon the algorithm. Criterion is disabled
##  if maxeval is non-positive.
## 
## maxtime
##  possible values: maxtime &gt; 0
##  default value:   -1.0
## 
##  Stop when the optimization time (in seconds) exceeds maxtime. This is
##  not a strict maximum: the time may exceed maxtime slightly, depending
##  upon the algorithm and on how slow your function evaluation is.
##  Criterion is disabled if maxtime is non-positive (default).
## 
## tol_constraints_ineq
##  possible values: tol_constraints_ineq &gt; 0.0
##  default value:   rep( 1e-8, num_constraints_ineq )
## 
##  The parameter tol_constraints_ineq is a vector of tolerances. Each
##  tolerance corresponds to one of the inequality constraints. The
##  tolerance is used for the purpose of stopping criteria only: a point
##  x is considered feasible for judging whether to stop the optimization
##  if eval_g_ineq(x) &lt;= tol. A tolerance of zero means that NLopt will
##  try not to consider any x to be converged unless eval_g_ineq(x) is
##  strictly non-positive; generally, at least a small positive tolerance
##  is advisable to reduce sensitivity to rounding errors. By default the
##  tolerances for all inequality constraints are set to 1e-8.
## 
## tol_constraints_eq
##  possible values: tol_constraints_eq &gt; 0.0
##  default value:   rep( 1e-8, num_constraints_eq )
## 
##  The parameter tol_constraints_eq is a vector of tolerances. Each
##  tolerance corresponds to one of the equality constraints. The
##  tolerance is used for the purpose of stopping criteria only: a point
##  x is considered feasible for judging whether to stop the optimization
##  if abs( eval_g_ineq(x) ) &lt;= tol. For equality constraints, a small
##  positive tolerance is strongly advised in order to allow NLopt to
##  converge even if the equality constraint is slightly nonzero. By
##  default the tolerances for all equality constraints are set to 1e-8.
## 
## print_level
##  possible values: 0, 1, 2, or 3
##  default value:   0
## 
##  The option print_level controls how much output is shown during the
##  optimization process. Possible values: 0 (default): no output; 1:
##  show iteration number and value of objective function; 2: 1 + show
##  value of (in)equalities; 3: 2 + show value of controls.
## 
## check_derivatives
##  possible values: TRUE or FALSE
##  default value:   FALSE
## 
##  The option check_derivatives can be activated to compare the
##  user-supplied analytic gradients with finite difference
##  approximations.
## 
## check_derivatives_tol
##  possible values: check_derivatives_tol &gt; 0.0
##  default value:   1e-04
## 
##  The option check_derivatives_tol determines when a difference between
##  an analytic gradient and its finite difference approximation is
##  flagged as an error.
## 
## check_derivatives_print
##  possible values: 'none', 'all', 'errors',
##  default value:   all
## 
##  The option check_derivatives_print controls the output of the
##  derivative checker (if check_derivatives==TRUE). All comparisons are
##  shown ('all'), only those comparisions that resulted in an error
##  ('error'), or only the number of errors is shown ('none').
## 
## print_options_doc
##  possible values: TRUE or FALSE
##  default value:   FALSE
## 
##  If TRUE, a description of all options and their current and default
##  values is printed to the screen.
## 
## population
##  possible values: population is a positive integer
##  default value:   0
## 
##  Several of the stochastic search algorithms (e.g., CRS, MLSL, and
##  ISRES) start by generating some initial population of random points
##  x. By default, this initial population size is chosen heuristically
##  in some algorithm-specific way, but the initial population can by
##  changed by setting a positive integer value for population. A
##  population of zero implies that the heuristic default will be used.
## 
## ranseed
##  possible values: ranseed is a positive integer
##  default value:   0
## 
##  For stochastic optimization algorithms, pseudorandom numbers are
##  generated. Set the random seed using ranseed if you want to use a
##  'deterministic' sequence of pseudorandom numbers, i.e. the same
##  sequence from run to run. If ranseed is 0 (default), the seed for the
##  random numbers is generated from the system time, so that you will
##  get a different sequence of pseudorandom numbers each time you run
##  your program.</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-NLopt:website">
<p>Johnson, Steven G. n.d. “The NLopt Nonlinear-Optimization Package.” <a href="http://ab-initio.mit.edu/nlopt">http://ab-initio.mit.edu/nlopt</a>.</p>
</div>
<div id="ref-LiuNocedal:1989">
<p>Liu, D. C., and J. Nocedal. 1989. “On the Limited Memory BFGS Method for Large Scale Optimization.” <em>Math. Programming</em> 45: 503–28.</p>
</div>
<div id="ref-Nocedal:1980">
<p>Nocedal, J. 1980. “Updating Quasi-Newton Matrices with Limited Storage.” <em>Math. Comput.</em> 35: 773–82.</p>
</div>
<div id="ref-Powell:1994">
<p>Powell, M. J. D. 1994. “A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation.” In <em>Advances in Optimization and Numerical Analysis</em>, edited by S. Gomez and J.-P. Hennart, 51–67. Kluwer Academic, Dordrecht.</p>
</div>
<div id="ref-Powell:1998">
<p>———. 1998. “Direct Search Algorithms for Optimization Calculations.” <em>Acta Numerica</em> 7: 287–336.</p>
</div>
<div id="ref-Svanberg:2002">
<p>Svanberg, Krister. 2002. “A Class of Globally Convergent Optimization Methods Based on Conservative Convex Separable Approximations.” <em>SIAM J. Optim.</em> 12 (2): 555–73.</p>
</div>
<div id="ref-Xie:2014">
<p>Xie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible Research in R.” In <em>Implementing Reproducible Computational Research</em>, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC. <a href="http://www.crcpress.com/product/isbn/9781466561595">http://www.crcpress.com/product/isbn/9781466561595</a>.</p>
</div>
<div id="ref-Xie:2015">
<p>———. 2015. <em>Dynamic Documents with R and Knitr</em>. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. <a href="http://yihui.name/knitr/">http://yihui.name/knitr/</a>.</p>
</div>
<div id="ref-Xie:2016">
<p>———. 2016. <em>Knitr: A General-Purpose Package for Dynamic Report Generation in R</em>. <a href="http://yihui.name/knitr/">http://yihui.name/knitr/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>This package should be considered in beta and comments about any aspect of the package are welcome. This document is an R vignette prepared with the aid of <code>knitr</code> <span class="citation">(Xie 2014, 2015, 2016)</span>. Financial support of the UK Economic and Social Research Council through a grant (RES-589-28-0001) to the ESRC Centre for Microdata Methods and Practice (CeMMAP) is gratefully acknowledged.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Jelmer Ypma, Steven G. Johnson, Aymeric Stamm.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
